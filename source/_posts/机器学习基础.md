---
title: 机器学习基础
date: 2024-03-28 19:45:13
mathjax: true
tags: 
  - 机器学习
  - AI
  - 算法
categories:
  - 技术
---

## 概述

机器学习算法作为人工智能的底层算法早已应用于各行各业，如：天气预测、人脸识别、物体识别追踪、数据的分类挖掘与分析、短视频平台的推搜机制、金融风险预测与监控、机器翻译等等。掌握机器学习的基础算法对进一步提升深度学习、强化学习等人工智能专业技术有重要的意义。

本文旨在讲述传统的机器学习算法的底层原理，让你更好地理解机器学习算法工作的机制。

### 机器学习的两个基本应用方向

- 回归(预测)
- 分类

### 核心思想

根据要执行的任务类别(回归、分类等)来构建不同的预测函数，选择不同的代价函数，通过输入数据集训练模型来帮助代价函数一步一步调整预测函数的参数，最终确定最优的参数(权重、偏置)，进而确定最终的预测函数公式，完成训练即可投入应用。

简而言之，模型训练是一个对预先选择好的代价函数，通过向其不断输入数据集（输入x输出y来找到最小的y所对应x的参数），帮助代价函数反推预测函数的过程。

通俗解释机器学习的实现逻辑

为了完成某一类或多类任务(如：预测任务、分类任务)，选择采用机器学习来帮助我们达成这一任务目标：

1. 构建样本数据集
   - 如：要识别动物中的鸟类，就构建一个庞大的动物数据集，其中要包含大量的鸟类数据样本与其他动物的数据样本

2. 选择数据样本的特征标签
   - 如：识别鸟类时，可能的特征标签包括：翅膀、羽毛、鸟嘴、飞翔等

3. 根据任务类别选择对应的算法
   - 即选择预测函数公式，如：想完成回归任务，可以选择线性回归公式

4. 根据样本特征标签调整算法
   - 根据样本特征数量对算法公式进行升降维等操作
   - 此时直接将样本特征输入算法模型会输出预测值，但这个值是不准确的，因为还没有根据实际任务对预测函数公式进行调整

5. 对模型进行调整优化
   - 选择合适的优化算法
   - 通过判断预测值与真实值之间的差距对模型公式不断进行调整
   - 当预测值与实际值间的差距非常小时，说明优化算法已经完成了对模型的优化

基础分类规则

机器学习模型需要根据具体需求选择大量数据进行训练以达到最优的效果。根据训练数据集样本是否有标签，可以将机器学习算法分为：

- 有监督学习
- 无监督学习
- ...

### 逻辑链路

1. 选择代价函数：根据要执行的任务类别（回归或分类）和具体需求，选择合适的代价函数

2. 初始化模型参数：在训练开始前，初始化模型的参数（如w权重和b偏置）

3. 输入数据集：将数据集输入到模型中，模型使用当前参数进行预测

4. 计算损失：使用代价函数计算模型预测值与实际值之间的差异

5. 参数调整：通过优化算法（如梯度下降）根据损失值来调整模型参数

6. 重复迭代：重复上述步骤，每次迭代都会更新模型的参数

7. 找到最小损失：经过足够多次的迭代后，模型参数会趋于稳定，确定最终的预测函数

## 有监督学习

有明确标签作为学习目标的算法即为有监督学习算法。回归和分类任务是有监督学习算法的基础应用领域。

### 回归算法

回归算法也叫预测算法，通常用来做预测推理应用，最常见的是线性回归算法。

线性回归

任务：设计一个算法去拟合有因果关系的数据

##### 预测原理

因为数据点遵从正比例分布，所以这样的分布规律可以用形如 $y=wx+b$ (预测函数)的公式去拟合，只需要算出w和b的值。

将训练集输入模型(function，简称f)训练后，对模型f输入x，模型根据预测函数f(x)给出预测结果 $\hat{y}$ ($\hat{y}$ 因为是模型预测的y值，所以可能会与真实的y值有误差)。

#### 预测函数(模型函数)：
- $f(x) = wx + b$

注：w为斜率，在机器学习中叫权重，b为截距，在机器学习中叫偏置

- 案例分析

- 简单的线性回归问题(未离散)
```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# 创建数组，对应x, y的坐标
x = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# 创建线性回归模型
model = LinearRegression()
model.fit(x, y)

# 使用模型预测新的x对应的y
x_predict = np.array([[6], [7], [8]])
y_predict = model.predict(x_predict)

# 可视化结果
plt.scatter(x, y, color='blue', label='Original coordinate data')
plt.plot(x, model.predict(x), color='red', label='fit line')
plt.scatter(x_predict, y_predict, color='green', label='Predicting coordinate data')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Simple linear regression model')
plt.legend()
plt.show()

# 返回模型斜率和截距
print(model.coef_[0], model.intercept_)
```
- 简单的线性回归问题(离散)
```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# 创建数组，对应x, y的坐标
x = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]])
y = np.array([2, 8, 6, 12, 7, 9, 14, 3, 5, 19, 10])

# 创建线性回归模型
model = LinearRegression()
model.fit(x, y)

# 使用模型预测新的x对应的y
x_predict = np.array([[12], [13], [14]])
y_predict = model.predict(x_predict)

# 可视化结果
plt.scatter(x, y, color='blue', label='Original coordinate data')
plt.plot(x, model.predict(x), color='red', label='fit line')
plt.scatter(x_predict, y_predict, color='green', label='Predicting coordinate data')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Simple linear regression model')
plt.legend()
plt.show()

# 返回模型斜率和截距
print(model.coef_[0], model.intercept_)
```

- 简单的线性回归问题(未离散)
```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# 创建数组，对应x, y的坐标
x = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]])
y = np.array([2, 8, 6, 12, 7, 9, 14, 3, 5, 19, 10])

# 创建线性回归模型
model = LinearRegression()
model.fit(x, y)

# 使用模型预测新的x对应的y
x_predict = np.array([[12], [13], [14]])
y_predict = model.predict(x_predict)

# 可视化结果
plt.scatter(x, y, color='blue', label='Original coordinate data')
plt.plot(x, model.predict(x), color='red', label='fit line')
plt.scatter(x_predict, y_predict, color='green', label='Predicting coordinate data')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Simple linear regression model')
plt.legend()
plt.show()

# 返回模型斜率和截距
print(model.coef_[0], model.intercept_)
```
###损失函数
上面我们提到为了去拟合有因果关系的数据样本，需要设计一个预测函数，设计方法是：先随机的选一条过原点的直线，计算直线与所有样本点的偏离程度，再根据误差大小调整直线的斜率w。那么我们如何去量化各个样本数据的偏离程度(误差)以及模型整体预测的精准度呢？
以$$y=wx$$为例：
我们需要用到代价函数(最常用的是均方误差：也就是计算所有样本误差平方和的平均值)
代价函数$$J(x)$$提供了一种衡量模型预测整体样本精确度的方法
损失函数$$e_j$$提供了一种方法来衡量模型对 单个样本 预测的结果与实际结果之间的差距。
其中代价函数的结果等于所有样本的损失函数结果之和除样本数量。

根据以上案例可以看出，图中的散点周围存在一条红色的线。这是由线性回归算法的预测函数所表示的拟合线，拟合线上的每一点代表了模型的预测值，但往往预测值与实际值之间是有gap的，这个gap叫误差。

理论上可以通过观察误差来看出模型预测的精准性，但是每个预测值与实际值之间的误差是不同的，如何去衡量模型的预测精准性呢？

在线性回归中，常用的损失函数是均方误差（Mean Squared Error, MSE）运用了最小二乘法的思想，它计算了模型预测值与实际值之间差异的平方的平均值。损失函数的值越小，表示模型的预测越准确。

注：除以2是为了方便计算

#### 推导公式
- $$e_1=(y_1-w*w_1)^2=(w*w_1-y_1)^2$$
- $$e_1=w^2*x_1^2-2(w*x_1*y_1)+y_1^2=x_1^2*w^2-2(x_1*y_1)*w+y_1^2$$

#### 损失函数公式
- $$e_j=x_j^2*w^2-2(x_j*y_j)*w+y_j^2$$

#### 代价函数
与损失函数类似用均方误差来衡量预测值与实际值间的差异，但代价函数是预测 所有样本 损失的平均值，用于衡量模型在整个训练集上的表现。
代价函数的结果等于所有样本的损失函数结果和除样本数。

#### 推导公式
将所有样本数据的误差平方e相加平均化求模型的整体代价函数：
- $$e=\frac{1}{n}((x_1^2+...+x_n^2)*w^2+(-2*x_1*y_1-...-2*x_n*y_n)*w+(y_1^2+...+y_n^2))$$
为求 $$w$$ 可以把$$w$$前包含$$x$$和$$y$$的项当作系数，将公式简化：
- $$e=a*w^2+b*w+c$$ 
$$a>0$$
通过以上的式子，我们能发现这是个一元二次方程式，其中抛物线最低点$$y$$时模型损失值最小，参数取值$$w$$所拟合出来的直线(预测函数)对样本的预测是最精准的

  但是大多数情况下预测函数不只是简单的在二维平面上过原点的直线函数$$y=wx$$，如果再对预测函数加上偏置$$b$$那么预测函数就会由一个未知数变为两个未知数，模型也就由二维平面的抛物线变为三维空间的 碗装抛物面模型。这时就要找到抛物面的最低点
预测函数的w与b难以确定，而代价函数的值是由两个变量w、b共同决定的，所以实际上的代价函数图像是一个三维的图像。

#### 代价函数公式
- $$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)}）-y^{(i)})^2$$

代价函数与预测函数
上面提到了线性回归的预测函数公式的形式为:
$$f=wx+b$$
那么我们怎样去找到预测函数的预测精准度最优时的参数w呢？
根据代价函数值最小时预测精准度最高的原则，只需要找到代价函数值最小时的横坐标值即可，如下图y=x的预测函数横坐标值

下面我们用一个简单的案例展示下代价函数的可视化图像
```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 创建一个简单的线性回归样本数据
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 3, 4, 7, 11])

# 假设模型为 y=wx+b, 计算代价函数 MSE = (1/n) * sum((y - (wx+b))**2)
# 定义代价函数
def compute_loss(w, b, x, y):
    n = len(x)
    loss = (1/n) * np.sum((y - (w*x + b))**2)
    return loss

# 生成 w 和 b 的值
w_values = np.linspace(-1, 3, 400)
b_values = np.linspace(-1, 3, 400)

# 创建w 和 b 的网络
w, b = np.meshgrid(w_values, b_values)

# 计算每一个(w, b) 对的损失
loss = np.zeros(w.shape)
for i in range(len(w_values)):
    for j in range(len(b_values)):
        loss[i, j] = compute_loss(w[i, j], b[i, j], x, y)

# 绘制三维的代价函数
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(w, b, loss,
                       cmap='viridis', edgecolor='none')
ax.set_xlabel('weight w')
ax.set_ylabel('bias b')
ax.set_zlabel('loss')
ax.set_title('Loss Surface')
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()

# 绘制代价函数的等高线图
plt.figure(figsize=(8, 6))
contour = plt.contour(w, b, loss, levels=np.logspace(-2, 3, 20), cmap='viridis')
plt.xlabel('Weight w')
plt.ylabel('Bias b')
plt.title('Loss Contour')
plt.clabel(contour, inline=True, fontsize=8)
plt.show()
```

### 梯度下降算法

---
梯度下降算法是人工智能领域极其重要的优化算法

上面我们提到了线性回归算法的代价函数是由预测函数(模型函数)的w(权重)、b(偏置)两个变量共同决定的，那么我们怎么去确定模型预测效果最优(代价函数最小)时的w与b值呢？
在实际的应用场景中，我们往往会用到不同的机器学习(包括深度学习)算法去完成回归、分类等任务，其预测函数可能比线性回归的预测函数复杂的多，相应的代价函数也会变得更加复杂，这其中可能会涉及到多个未知变量。如：J(w1,w2,w3,...,wn,b)。如下图就是一个神经网络算法的代价函数，那么我们该怎么去找到当代价函数的值达到最小时，所对应的w和b呢？

#### 作用
- 调整模型参数w、b(这是模型训练的过程)，以用来确定最终的预测函数公式

#### 核心思想
通过找到代价函数的最小值来实现对数据的最优拟合，选择了合适的代价函数，模型通过梯度下降算法更新参数，来向代价函数所设定的方向去学习。

以上图代价函数的三维模型为例，最终要求的是模型的最低点也就是谷底(代价函数的最小值)坐标。

具象化解释：在确定最终目标为找到谷底坐标后，先随机选一点，环绕这个点360度的方向，根据"走一步下降的高度最多"的原则，选择最快下坡的路径，最终找到的就是局部区域的谷底(也就是局部最小值)

#### 逻辑链路
通过代价函数对模型整体损失值误差的抽象，我们只需找到一元二次函数的最低点对应的$$w$$值即可，以下是梯度下降实现的具体逻辑链路
1. 确定代价函数；
2. 选择起始点；
3. 计算梯度；
4. 根据学习率前进下降；
5. 第三步和第四步重复迭代，直至找到最低点；

首先随意设置一个初始参数值(最常见的是将w、b参数都设为0)，对应图中就是随意选择一个坐标点。根据参数更新公式，不断迭代参数值，带入代价函数j(w,b)中计算代价函数的值，不断迭代调整，最终计算出代价函数的最小值

#### 梯度下降(参数更新)公式
- $$w = w - α \frac{∂}{∂w}J(w,b)$$
- $$b = b - α \frac{∂}{∂b}J(w,b)$$
注：α表示学习率，通常是0~1之间的一个正小数，用来控制下坡的步幅，α越小下坡步幅越小

$$\frac{∂}{∂w}J(w,b)$$
与 
$$\frac{∂}{∂b}J(w,b)$$
用来控制下坡的方向
$$∂\frac{∂}{∂w}J(w,b)$$
与 
$$∂\frac{∂}{∂b}J(w,b)$$
共同控制下坡的方向及步幅

w和b的值必须同时更新！
更新逻辑
正确（同时更新）
错误（非同时更新）
$$tmp_w = w - α  \frac{∂}{∂w}J(w,b)$$
$$tmp_b = b - α \frac{∂}{∂b}J(w,b)$$
$$w = tmp_w$$
$$b = tmp_b$$
$$tmp_w = w - α \frac{∂}{∂w}J(w,b)$$
$$w = tmp_w$$
$$tmp_b = b - α \frac{∂}{∂b}J(w,b)$$
$$b = tmp_b$$

### 梯度下降变体
#### 批量(batch)梯度下降(BGD)
每次更新参数时，都要用整个训练集来计算代价函数的梯度，意味着它在更新每一步的参数前都会考虑所有的训练样本。具有梯度计算准确的特点，但当训练集非常大时，每次更新都会很慢，计算代价高。
在每次更新参数时，需要看周围所有方向所有路径的最优解。
优点：保证算法的精准度，可以找到全局最优解
缺点：训练速度极慢，效率低

#### 随机梯度下降(SGD)
每次更新参数时，只用一个样本来计算代价函数的梯度，意味着它在更新每一步的参数前都仅仅只用到一个样本计算训练模型。具有梯度计算速度快的特点，但会让模型精准度变差。
在每次更新参数时，只看周围所有方向最近的一个样本点进行路径的筛选。
优点：训练速度快
缺点：算法的精准度差，难以找到全局最优解

#### 小批量梯度下降(MBGD)
每次更新参数时，用一小批样本来计算代价函数的梯度，意味着它在更新每一步的参数前选用一小批样本来计算训练模型。综合了批量和随机下降的优点，具精准度高和速度快的特点。
在每次更新参数时，只看周围所有方向最近的一批样本点进行路径的筛选。
优点：训练速度快，算法的精准度高
缺点：不如批量算法精准且不如随机算法快，并且依然难以找到全局最优解

### 多类特征预测(多元线性回归)
当一个样本集数据具有多个特征时，如何去进行回归预测？

核心思想：向量化(矢量化) 
  将多个特征作为向量输入模型进行预测
变量说明：
$$x_j$$ 
表示数据的第j个特征
$$n$$ 
表示特征总数
$$向量x^{(i)}$$ 
表示特征向量(某个数据)
$$向量x_j^{(i)}$$ 
表示某个数据中的某个特征值

非向量化模型公式：

$$f_{w,b}(x) = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b$$

向量化模型公式:

$$f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$$

(行向量的点乘)

多元特征参数的向量表示：
$$\vec{w}= [w_1, w_2, w_3,\ldots,w_n]$$

多元特征的向量表示：
$$\vec{x} = [x_1, x_2, x_3,\ldots,x_n]$$

多元特征的预测函数公式：
- $$f_{\vec{w},b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$$  

预测函数公式Python code：
- f = np.dot(w, x) + b (numpy并行计算向量值，速率快)

#### 多元线性回归的梯度下降
梯度下降(参数更新)公式
$$w_j = w_j - α \frac{∂}{∂w_j}J(\vec{w},b)$$
$$b = b - α \frac{∂}{∂b}J(\vec{w},b)$$

#### 特征缩放
在多个特征情况下，存在某个或某几个特征值的数据量级与其他特征值差异较大时，x轴y轴量级差异大，图像难以观测，则需利用缩放公式对特征进行缩放处理，如：预测房价与房子面积及卧室数量间的关系。

#### 缩放方法1：归一化
缩放公式：
- $$x_1=x_1/最大边界值$$

如：房屋面积数据范围$$100≤x_1≤2000$$，卧室数量数据范围$$0≤x_2≤5$$
$$x_1缩放=x_1/2000$$
$$x_2缩放=x_2/5$$   
得：$$0.15<=x_1缩放<=1$$, 
$$0<=x_2缩放<=1$$

#### 缩放方法2：平均归一化
先求训练集上特征$$x_1$$的平均值$$μ_1$$
缩放公式：
$$x_1=\frac{(x_1 - μ_1)}{(最大边界值-最小边界值)}$$

例如：房屋面积：$$300≤x_1≤2000$$  卧室数量：$$0≤x_2≤5$$
假设$$x_1$$的均值$$μ_1$$是300，$$x_1=\frac{(x_1 - μ_1)}{(2000-300)}$$,   得 $$-0.18≤x_1≤0.82$$
假设$$x_2$$的均值$$μ_2$$是2.3，$$x_2=\frac{(x_2 - μ_2)}{(5-0)}$$,   得 $$-0.46≤x_2≤0.54$$

#### 缩放方法3：Z分数标准化
统计学中的中心极限定理告诉我们，无论一个总体分布如何，只要样本量足够大，样本均值的分布将近似正态分布。这意味着在很多情况下，即使原始数据不是正态分布的，其特征的均值也倾向于形成正态分布，使得正态分布在处理问题时非常有用
因为样本特征分布遵循正态分布规律，先计算每个特征的标准差$$σ$$
缩放公式：
  $$x_1=\frac{(x_1-μ_1)}{σ_1}$$

例如：房屋面积：$$300≤x_1≤2000$$  卧室数量：$$0≤x_2≤5$$
假设$$x_1$$的均值$$μ_1$$是300、$$σ_1$$是450，$$x_1=\frac{(x_1 - μ_1)}{σ_1}$$,   得 $$-0.67≤x_1≤3.1$$
假设$$x_2$$的均值$$μ_2$$是2.3、$$σ_2$$是1.4，$$x_2=\frac{(x_2 - μ_2)}{σ_2}$$,   得 $$-1.6≤x_2≤1.9$$

#### 检查梯度下降是否收敛
在执行梯度下降时，我们怎么去知道某个时刻模型刚好训练好，达到收敛效果呢？

在模型训练时执行每次梯度下降后，$$J(w, b)$$值应该是减少的；若执行多次梯度下降后，$$J(w, b)$$值变化不大(趋于水平)时，则说明此时的梯度下降收敛，训练可以结束了；若执行某次梯度下降后，$$J(w, b)$$值变大了，则说明$$α$$值(学习率)过大，应该减小$$α$$值。
检测方式：自动收敛测试
选定一个极小的数$$ξ$$作为检测梯度下降是否收敛的检验值，若$$J(w, b)_0 - J(w, b)_1≤ξ$$ 则说明梯度下降收敛，停止模型训练

### 学习率的选择
如果学习率太小，那么梯度下降会起作用，但下降速度会很慢，需要很长时间，很多步骤才可以接近最小值
如果学习率太大，则参数更新可能会过冲，并且永远不会达到最小值，无法收敛甚至可能会发散

#### 核心逻辑
以0.001、0.003、0.01、0.03、0.1、0.3、1 这7个节点作为调整值，根据训练速度及是否收敛发散做调整

### 特征工程
特征的选择对于模型的性能有巨大的影响，为了让学习算法更容易的做出准确预测，引入了特征工程来寻找最优特征

#### 核心逻辑：
通过人工手动选择结合问题的原始特征来创造和定义新特征来尽可能地得到与模型输出值强相关的特征
如：为了准确预测房价，目前已有的原始特征为房子的长($$x_1$$)、宽($$x_2$$)、地基深度($$x_3$$)，引入通过将原始特征房子长($$x_1$$)与 宽($$x_2$$) 与 地基深度($$x_3$$) 相乘得到房子空间体积($$x_4$$)作为新特征，预测房价与房子长度、宽度、地基深度、房子空间体积的关系。得到一个高性能房价预测模型
$$f_w, _b(\vec{x}) = w_1x_1+w_2x_2+w_3x_3+w_4x_4+b$$

### 拟合曲线
上面我们利用线性回归算法来进行回归预测，数据样本通常分布状态像一条均匀的直线，算法模型最终拟合的是一条直线，那么如果数据样本以曲线形态分布或者不规则形态分布，该怎么去拟合曲线呢？

#### 核心思想
多项式函数变换(注：需深刻理解多项式函数的数学意义及形态)
通过将原始的预测函数调整为多项式函数，以实现复杂的拟合应用(适用于分类等其他算法模型公式)
$$f_w,_b(\vec{x})=w_1x_1+w_2x_2²+w_3x_3^3+...+w_nx_n^n+b$$
这里的坑较多，需格外注意，若多项式变换错误，则会导致模型性能差，还会导致过拟合、欠拟合问题

如上图，若使用二次项模型公式拟合曲线，在房屋面积增大到一定程度后，模型所预测出的房价会大幅下降。根据常识判断这样的结论显然是不合理的，所以这里需要用到平方根或三次项模型公式进行拟合。
注：拟合曲线前需对特征进行缩放
Scikit-learn：线性回归开源的python工具包

## 分类算法
常见的分类算法有逻辑回归算法

---
#### 核心思想
以二进制（1 或 0 代表 Yes or No）来对数据进行分类，将数据样本分为两类，如下图
传统的线性回归算法不适用于分类问题

从上图中可以直观发现：样本量增加前，或许线性回归算法所拟合的直线可以(通过图中垂直于x轴的蓝色直线)间接的将两类不同特征的数据样本区分开（可以在线性回归算法所拟合的直线上取0-1的中间值y=0.5，基于y所对应的横坐标x，经过点(x,0)做垂直于x轴的直线L将样本分类）
但随着样本数据量的增加，拟合线的斜率会衰减，0-1的中间值y=0.5所对应的x坐标会向x轴的正方向移动，且随着样本量的不断增加不断向后移动，造成严重的错误分类

在线性回归算法章节我们介绍了线性回归算法是来用做预测推理任务的，线性回归算法模型输出的是连续值，它可以通过拟合一条连续的直线或曲线，根据已有的数据样本预测出未知的数据规律。同样在分类任务中线性回归算法也可以拟合一条连续的直线或曲线(取值范围为整个实数集)，可以将具有不同特征的数据样本通过拟合线划分开，实现分类效果，如下图。

那么为什么线性回归算法不能用做分类任务呢？
#### 核心原因
因为分类任务需要输出的结果是：数据样本x属于某个类别的概率（百分数），输出值介于0-1之间，但线性回归算法输出值的取值范围为整个实数集，无法给出一个量化的分类结论来解释分类效果。所以线性回归算法不适用于分类任务。

#### 思考
在分类任务中我们需要输出的结果是某个数据样本属于某个特征类别的概率，所以需要将输出的连续特征值映射在0-1之间才可解释分类的效果。

### 逻辑(logistic)回归算法

#### 期望
以1 和 0（Yes or No）作为样本特征的两个类别，通过算法模型拟合一条线，来将样本的特征值分布在0-1之间，以表示某个样本具某类特征的概率，直观的表示出整体数据集样本的特征概率分布，期望达到类似下图的分类效果：

为了达到上图分类的效果，我们需要引入sigmoid函数，将输入的数据抽象为类似上图取值范围介于0-1之间图像

#### Sigmoid函数
普遍用于分类算法的函数,函数图像一般类似上图，输出值为0-1之间的小数(概率)
$$g(z)=\frac{1}{(1+e^{—z})}$$ 
$$0< g(z)<1$$

但Sigmoid函数本身只是一个概率映射函数，将任意实数映射到(0,1)之间，代表概率。如果直接将样本特征输入Sigmoid函数，模型将无法学习特征之间的复杂关系和相互作用，因为这种方法没有考虑到特征的权重和组合。Sigmoid函数进行分类任务时输入的z值应该是一个可以代表数据样本集内所有特征的值。

所以以什么值作为Sigmoid函数的输入值(z值)呢?
让我们重新审视一下 线性回归算法所代表的意义
线性回归算法的输出值，通常表示为 $$y =  w_1x_1 + w_2x_2^2 + ... + w_nx_n^n+b$$ 是对实际值$$y$$的预测或估计。这个值是通过将每个样本特征 $$x_i$$乘以相应的系数 $$w_i$$（表示特征 $$x_i$$ 对 $$y$$ 的影响程度）并加上一个截距$$b$$ 计算得出的。线性回归模型试图找到每个样本特征系数的系数值，使得模型预测的$$y$$值和实际观察到的 $$y$$ 值之间的差距最小。也就是说线性回归算法输出值是样本特征的加权和，根据数据样本集中所有已设定好的样本特征，综合这些特征预测拟合出一个结果。
线性回归预测输出的意义在于，它提供了一个连续值的预测，这个预测反映了自变量(样本特征)和因变量(预测值)之间的线性关系。这个结果可以定义将样本集按特征类别划分的拟合曲线和特征权重的综合，给以模型可解释性。

所以将线性回归算法输出的结果作为Sigmoid函数的输入值来完成分类任务是符合期望的
通过将线性回归算法的输出值作为Sigmoid函数的输入，转化成概率，这种转化操作让线性回归模型的输出可以被解释为特定类别的发生概率，这是逻辑回归用于分类问题的关键
例如：想根据肿瘤的特征来预测肿瘤是否癌变，这其实是一个二分类问题(癌变与否)，这里使用逻辑回归，在这个过程中结合线性回归和Sigmoid函数，以下是详细的实现逻辑：
1. 首先进行线性回归计算，基于肿瘤特征(大小、形状、细胞状态等)，线性回归模型会计算出一个线性组合值，这个值表示了肿瘤特征的加权和
2. 将这个值(z值)用作Sigmoid函数的输入，Sigmoid函数将这个线性回归输出的实数值(z值)映射到(0,1)区间内的一个值，这个值可以解释为概率。
3. 这里Sigmoid输出的概率值就是肿瘤癌变的预测概率。当这个值高于某个阈值(如：0.5)时，模型可能预测肿瘤为恶性(癌变)；当概率低于这个阈值时，模型可能会预测肿瘤为良性。

通过将线性回归算法的输出值作为Sigmoid函数的输入，转化成概率，这种转化操作让线性回归模型的输出可以被解释为特定类别的发生概率，这是逻辑回归用于分类问题的关键。以下是公式实现逻辑
步骤一：通过线性回归算法计算样本特征的加权和Z
 $$Z= f_{\vec{w},b}(\vec{x})=\vec{w}*\vec{x}+b$$

步骤二：将特征加权和Z值输入Sigmoid函数(概率映射函数)计算：
$$g(z)=\frac{1}{1+e^{-z}}$$

#### 模型公式
$$f_{\vec{w}, b}(\vec{x})=g(\vec{w}*\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}*\vec{x}+b)}}$$
即：
$$f_{\vec{w}, b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}*\vec{x}+b)}}$$

对肿瘤的癌变与否预测，若$$f_{\vec{w}, b}(\vec{x})=0.7$$，则表示肿瘤恶化的概率是70%，用$$P(y=1)=0.7$$表示，
那么肿瘤未癌变的概率即未得癌症的概率是30%，用$$P(y=0)=0.3$$表示，$$P(y=1)+P(y=0)=1$$

概率表示
$$f_{\vec{w},b}(\vec{x})=P(y=1 | \vec{x};\vec{w},b)$$
注：其中$$P(y=1 | \vec{x};\vec{w},b)$$ 表示在 $$\vec{w}$$ 和 $$b$$ 的影响下y=1的概率是多少

#### 决策边界
在前面我们提到了分类任务中线性回归算法可以通过拟合一条连续的直线或曲线(取值范围为整个实数集)，将具有不同特征的数据样本通过拟合线划分开，如下图实现分类效果。

这条可以将两个具有不同特征的样本划分开的拟合线就是决策边界
怎么去理解决策边界的数学意义？

Sigmoid函数将特征的加权和映射在0-1之间的概率区间内，根据Sigmoid函数公式：$$g(z)=\frac{1}{(1+e^{—z})}$$ 
当Sigmoid函数输入值 $$z=0$$  时，即线性回归输出的特征加权和 $$\vec{w}*\vec{x}+b=0$$ 时 $$y=g(0)=0.5$$ 
正好位于0-1的中间值 为边界值，所以 $$\vec{w}*\vec{x}+b=0$$ 为决策边界

例:
从上面的结论中可以发现，线性回归算法在分类模型中的作用是找到决策边界，拟合一条连续的直线或曲线将样本进行分类

#### 损失函数

i=1,2,...,m；j=1,2,...,n
其中i是训练样本数，j是特征数
通过 $$f_{\vec{w}, b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}*\vec{x}+b)}}$$ 来把样本分为0或1

那么怎么去找到 $$\vec{w}=[w_1,w_2,w_3,...,w_n]$$ 和 $$b$$
线性回归算法的损失函数是利用均方误差算法计算的
$$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})^2$$

逻辑回归沿用线性回归损失函数(均方误差)会造成有多个局部最小值，无法梯度下降获取全局最小值

那么逻辑回归算法的损失函数该如何选择呢？
假设损失函数为  $$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$$ 表示预测值 $$\hat{y}$$ 与真实的 $$y$$ 的误差
我们选择了一条适用于逻辑回归的损失函数，其值$$loss$$公式如下:
注：通常情况下损失函数都是预先定义好的直接使用即可，但想要搞清楚算法在处理不同任务时模型用到的损失函数需要较高的数学能力，去理解损失函数的数学意义。

损失函数公式：
$$L(f_{\vec{w},b}(\vec x^{(i)},y^{(i)})=-y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))$$

#### 代价函数
代价函数公式推导：
步骤一：$$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec x^{(i)},y^{(i)}))$$

步骤二：$$L(f_{\vec{w},b}(\vec x^{(i)},y^{(i)})=-y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))$$

步骤三：$$J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}L(f_{\vec{w},b}(\vec x^{(i)},y^{(i)}))=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]$$

#### 代价函数公式
$$J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]$$

#### 梯度下降
目标：找到最优的$$\vec{w}$$，$$b$$值（$$J_{\vec{w},b}$$最小时对应的$$\vec{w}$$，$$b$$）
梯度下降公式推导：
步骤一：$$w_j = w_j - α \frac{∂}{∂w_j}J(\vec{w},b)$$，$$b = b - α  \frac{∂}{∂b}J(\vec{w},b)$$
步骤二：$$\frac{∂}{∂w_j}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}$$，$$\frac{∂}{∂b}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$

#### 梯度下降(参数更新)公式
$$w_j=w_j-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]$$
$$b=b-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})]$$
注：可用矢量化和特征缩放加速梯度下降参数更新

#### 过拟合
- 原因：模型在当前训练集下训练过度，导致模型在新数据集下的表现力差、泛化能力差。
- 特征：过拟合的预测函数通常极其复杂，含高阶未知数x，且高阶未知数有很大的参数值w(权重)。模型的拟合曲线边界线通常非常崎岖，有很多的摆动。
如：准备一个动物样本的训练集对模型训练，希望达到可以判断该动物是否是鸟类的效果。最终训练后的模型可以精准识别当前数据集内的动物是否为鸟类，且识别准确度达100%，但模型对一个全新的动物样本数据集进行识别，结果显示识别的准确度低，

#### 解决方式
- 方法1：准备更多的数据样本大量训练模型
- 方法2：减少样本的特征数，用较少且与y值强相关的特征子集训练模型
- 方法3：正则化

正则化：通过引入惩罚项"$$λ$$"，惩罚所有样本的特征参数值$$w_j$$，鼓励学习算法收缩参数值，温和地减少一些特征的影响
如：准备一个有200个动物样本的训练集对模型训练，样本特征有 飞翔 $$x_1$$ 翅膀 $$x_2$$ 尖嘴 $$x_3$$ 羽毛 $$x_4$$ 尾巴 $$x_5$$ 爪子$$x_6$$ 眼睛 $$x_7$$，走路 $$x_8$$ 吃植物 $$x_9$$ 希望达到可以判断该动物是否是鸟类的效果。最终训练后的模型可以精准识别当前数据集内的动物是否为鸟类，且识别准确度达100%，但模型在一个全新的动物样本数据集进行识别，结果显示识别的准确度低。
  我们发现这个以上模型训练过程有几个问题：
  问题1：训练集的样本过少；
  问题2：样本的特征标签过多，且有些特征标签与鸟类特征非强相关，如：尾巴$$x_5$$、眼睛$$x_7$$，走路$$x_8$$、吃植物$$x_9$$，若这些样本标签有较高阶的幂和系数，则干扰过大。
  $$f(x)=28x_1-385x_2^2+39x_3^3-174x_4^4+258x_5^5+75x_6^6-352x_7^7+7x_8^8-35x_9^9+100$$
  正则化惩罚$$\downarrow$$
$$f(x)=17x_1-75x_2^2+11x_3^3-24x_4^4+21x_5^5+0.1x_6^6-0.002x_7^7+0.0001x_8^8-0.00001x_9^9+10$$

### 正则化
为什么要对所有的参数项施加惩罚？
上面提到针对过拟合的模型公式通常公式内含复杂的高阶未知数以及非常大的系数$$w_j$$值，导致模型的拟合线条异常崎岖，所以为避免过拟合，需减小模型公式中高阶未知数对整体函数的影响，需给予高阶未知数的参数项(权重)极小的值来减小高阶特征值对预测结果的影响，因为我们不知道所有参数中哪些是重要的，我们无法针对某个参数做惩罚(缩小权重)，所以正则化通过对所有参数项施加惩罚，来普遍的减小过拟合的风险，引入"$$λ$$"

#### 线性回归正则化
正则化代价函数
$$J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)})^2+\frac{λ}{2m}\sum_{j=1}^{n}w_j^2$$

正则化梯度下降(参数更新)公式
$$w_j=w_j-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}w_j]$$
$$b=b-α\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$
注意：并未对偏置$$b$$进行正则化

正则化梯度下降(参数更新)公式推导
步骤一：$$w_j=w_j-α\frac{∂}{∂w_j}J(\vec{w},b)$$
$$\frac{∂}{∂w_j}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}]+\frac{λ}{m}w_j$$

步骤二：$$b=b-α\frac{∂}{∂w_j}J(\vec{w},b)$$
$$\frac{∂}{∂b}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}[(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})]$$

步骤三：
$$w_j=w_j-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}w_j]$$
$$b=b-α\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$

求正则化$$\frac{∂}{∂w_j}J(\vec{w},b)$$偏导：
$$\frac{∂}{∂w_j}J(\vec{w},b)=\frac{∂}{∂w_j}[\frac{1}{2m}\sum_{i=1}^{m}(f(\vec{x}^{(i)})-y^{(i)})^2+\frac{λ}{2m}\sum_{j=1}^{n}w_j^2]$$
因为：
$$f(\vec{x}^{(i)})=\vec{w}\vec{x}^{(i)}+b$$
所以：
$$\frac{∂}{∂w_j}J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}[(\vec{w}\vec{x}^{(i)}+b-y^{(i)})2x_j^{(i)}]+\frac{λ}{2m}2w_j$$
$$=\frac{1}{m}\sum_{i=1}^{m}[(\vec{w}\vec{x}^{(i)}+b-y^{(i)})x_j^{(i)}]+\frac{λ}{m}*w_j$$
因为：
$$f(\vec{x}^{(i)})=\vec{w}\vec{x}^{(i)}+b$$
所以：
$$\frac{∂}{∂w_j}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}w_j$$

正则化数学意义 $$w_j$$ 梯度下降公式：

$$w_j=w_j-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}w_j]$$

拆解：
$$w_j=w_j-α\frac{λ}{m}w_j-α\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}$$
其中
- $$w_j-α\frac{λ}{m}w_j=w_j(1-α\frac{λ}{m})$$ 
- $$\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}=\frac{∂}{∂w_j}J(\vec{w},b)$$ 
是常规梯度下降公式组成

$$1-α\frac{λ}{m}$$ 
是一个常数，且数值很小，在梯度下降环节不变,其作用是在每次梯度下降时对$$w_j$$进行微调
如：学习率 $$α=0.01$$ 正则化惩罚系数 $$λ=1$$ 训练迭代(梯度下降)次数 $$m=50$$次
- $$α\frac{λ}{m}=0.01*\frac{1}{50}=0.0002$$
- $$w_j(1-0.0002)$$ 
每次迭代时参数 $$w_j$$ 都变为上一次迭代的0.9998倍(实现了对参数的微调)
对比常规的 $$w_j$$ 参数更新公式：

常规公式(未引入正则化系数) $$w_j=w_j-\frac{∂}{∂w_j}J(\vec{w},b)$$
引入正则化惩罚系数：$$w_j=(1-α\frac{λ}{m})w_j-α\frac{∂}{∂w_j}J(\vec{w},b)$$
注：正则化引入$$1-α\frac{λ}{m}$$系数对参数进行微调

#### 正则化逻辑回归

##### 代价函数
$$J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))+(1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))]+\frac{λ}{2m}\sum_{j=1}^{n}w_j^2$$

##### 梯度下降(参数更新)公式
$$w_j=w_j-α[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}w_j]$$
$$b=b-α\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})$$

