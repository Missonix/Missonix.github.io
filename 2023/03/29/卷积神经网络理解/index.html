<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>卷积神经网络理解 | Missonix</title><meta name="author" content="Missonix"><meta name="copyright" content="Missonix"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="卷积网络深度影响感受野大小卷积网络的深度会影响感受野的大小，但“感受的区域”和“感受的图片尺寸”是不同的概念。感受野是指网络中某个神经元所看到的输入图像的局部区域的大小，而图片尺寸是指输入图像的实际大小。随着卷积层数的增加，感受野的大小会逐渐增大，能够感知到更大的图像区域，从而有助于学习到更全局的特征。 采样1. 上下采样与通道维度上下采样（空间维度操作）：传统意义上的上下采样（如池化、插值）主要">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络理解">
<meta property="og:url" content="https://missonix.github.io/2023/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3/index.html">
<meta property="og:site_name" content="Missonix">
<meta property="og:description" content="卷积网络深度影响感受野大小卷积网络的深度会影响感受野的大小，但“感受的区域”和“感受的图片尺寸”是不同的概念。感受野是指网络中某个神经元所看到的输入图像的局部区域的大小，而图片尺寸是指输入图像的实际大小。随着卷积层数的增加，感受野的大小会逐渐增大，能够感知到更大的图像区域，从而有助于学习到更全局的特征。 采样1. 上下采样与通道维度上下采样（空间维度操作）：传统意义上的上下采样（如池化、插值）主要">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://missonix.github.io/img/Missonix.jpg">
<meta property="article:published_time" content="2023-03-29T00:26:13.000Z">
<meta property="article:modified_time" content="2025-05-11T06:44:47.951Z">
<meta property="article:author" content="Missonix">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://missonix.github.io/img/Missonix.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "卷积神经网络理解",
  "url": "https://missonix.github.io/2023/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3/",
  "image": "https://missonix.github.io/img/Missonix.jpg",
  "datePublished": "2023-03-29T00:26:13.000Z",
  "dateModified": "2025-05-11T06:44:47.951Z",
  "author": [
    {
      "@type": "Person",
      "name": "Missonix",
      "url": "https://missonix.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://missonix.github.io/2023/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '卷积神经网络理解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/Missonix.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Missonix</span></a><a class="nav-page-title" href="/"><span class="site-name">卷积神经网络理解</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">卷积神经网络理解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-29T00:26:13.000Z" title="发表于 2023-03-29 08:26:13">2023-03-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-11T06:44:47.951Z" title="更新于 2025-05-11 14:44:47">2025-05-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/">人工智能算法</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="卷积网络深度影响感受野大小"><a href="#卷积网络深度影响感受野大小" class="headerlink" title="卷积网络深度影响感受野大小"></a>卷积网络深度影响感受野大小</h2><p>卷积网络的深度会影响感受野的大小，但“感受的区域”和“感受的图片尺寸”是不同的概念。感受野是指网络中某个神经元所看到的输入图像的局部区域的大小，而图片尺寸是指输入图像的实际大小。<br>随着卷积层数的增加，感受野的大小会逐渐增大，能够感知到更大的图像区域，从而有助于学习到更全局的特征。</p>
<h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><h3 id="1-上下采样与通道维度"><a href="#1-上下采样与通道维度" class="headerlink" title="1. 上下采样与通道维度"></a>1. 上下采样与通道维度</h3><p>上下采样（空间维度操作）：传统意义上的上下采样（如池化、插值）主要调整图像的空间分辨率（行、列），不直接改变通道数。例如：</p>
<p>最大池化（Max Pooling）：仅减小空间尺寸，通道数不变。</p>
<p>双线性插值上采样：仅增大空间尺寸，通道数不变。</p>
<p>通道维度的变化：通道的升降维通常通过卷积操作（如1×1卷积）实现，而不是上下采样本身。但某些操作（如步长卷积）可能同时改变空间尺寸和通道数。</p>
<h3 id="2-卷积与下采样的关系"><a href="#2-卷积与下采样的关系" class="headerlink" title="2. 卷积与下采样的关系"></a>2. 卷积与下采样的关系</h3><p>常规卷积（无下采样）：若卷积步长（stride）为1，且输入输出尺寸相同（通过填充padding），则不涉及下采样。</p>
<p>下采样卷积：若步长&gt;1（如stride&#x3D;2），或通过池化（如MaxPool2d），则空间分辨率降低（如尺寸减半），实现下采样。</p>
<p>通道数的调整：卷积核数量（即输出的通道数）可独立设置。例如：</p>
<p>输入尺寸 [B, C_in, H, W] → 使用k个卷积核 → 输出 [B, k, H&#x2F;s, W&#x2F;s]（s为步长）。</p>
<p>此时空间分辨率降低（下采样），通道数可能增加（升维）或减少（降维）。</p>
<h3 id="3-通道维度升降的机制"><a href="#3-通道维度升降的机制" class="headerlink" title="3. 通道维度升降的机制"></a>3. 通道维度升降的机制</h3><p>通道升降维的核心方法：</p>
<p>卷积操作：通过调整卷积核数量（如从64个核变为128个核）改变通道数。</p>
<p>1×1卷积：专门用于通道维度的升&#x2F;降维（如ResNet中的“瓶颈结构”）。</p>
<p>全连接层（FC）：在某些网络（如VGG）中，FC层可能间接改变通道维度。</p>
<p>通道升维的目的：增加特征多样性，允许网络学习更复杂的特征组合（如不同通道对应不同边缘方向、颜色模式等）。</p>
<h3 id="4-下采样的目的"><a href="#4-下采样的目的" class="headerlink" title="4. 下采样的目的"></a>4. 下采样的目的</h3><p>关键作用：</p>
<p>扩大感受野（Receptive Field）：通过降低分辨率，后续层的每个神经元能覆盖输入图像的更大区域，从而捕捉全局特征。</p>
<p>减少计算量：空间尺寸减半可使计算量减少约4倍（面积平方关系）。</p>
<p>参数压缩与抽象化：通过逐层下采样，迫使网络逐步忽略局部细节，提取更高层次的语义特征（如从“边缘”到“物体部件”再到“完整物体”）。</p>
<h3 id="5-通道升维的目的"><a href="#5-通道升维的目的" class="headerlink" title="5. 通道升维的目的"></a>5. 通道升维的目的</h3><p>特征空间的扩展：</p>
<p>低维→高维：通过增加通道数，允许网络在不同通道中学习互补的特征（如颜色、纹理、形状）。</p>
<p>特征交互：多通道间的非线性组合（通过激活函数）能增强模型的表达能力（如SENet中的通道注意力）。</p>
<h3 id="6-网络架构的典型设计"><a href="#6-网络架构的典型设计" class="headerlink" title="6. 网络架构的典型设计"></a>6. 网络架构的典型设计</h3><p>经典模式：</p>
<p>编码器（Encoder）：逐步下采样（降低分辨率）并升维（增加通道数），提取抽象特征。</p>
<p>解码器（Decoder）：逐步上采样（恢复分辨率）并降维（减少通道数），用于分割、生成等任务。</p>
<p>全连接层（FC）的替代：现代网络（如ResNet、Transformer）倾向于用全局平均池化（GAP）替代FC层，避免参数爆炸。</p>
<p>激活函数的应用：</p>
<p>Sigmoid通常用于二分类输出层（概率映射），而非中间特征提取。</p>
<p>中间层多用ReLU或其变体（如Leaky ReLU）防止梯度消失。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通道维度变化与卷积的绑定：下采样（空间缩减）和通道升维（特征丰富化）常通过步长&gt;1的卷积同时实现。</p>
<p>下采样与参数量的关系：下采样主要减少计算量，但参数量可能因通道数增加而上升（如ResNet中每层的通道数翻倍）。</p>
<p>末端结构：现代网络设计更倾向于使用GAP+1×1卷积替代FC层，以提高泛化能力。</p>
<h2 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h2><h3 id="1-所有卷积核（如3×3、5×5）都能实现通道升维和下采样吗？"><a href="#1-所有卷积核（如3×3、5×5）都能实现通道升维和下采样吗？" class="headerlink" title="1. 所有卷积核（如3×3、5×5）都能实现通道升维和下采样吗？"></a>1. 所有卷积核（如3×3、5×5）都能实现通道升维和下采样吗？</h3><p>无论卷积核尺寸如何，通道数的调整由卷积核的数量决定，而下采样由步长（stride）控制。例如：</p>
<p>输入尺寸：[B, C_in, H, W]</p>
<p>卷积操作：使用 k 个尺寸为 K×K 的卷积核，步长 s</p>
<p>输出尺寸：[B, k, H&#x2F;s, W&#x2F;s]</p>
<p>无论 K 是1、3还是5，只要满足：</p>
<p>通道升维：k &gt; C_in（输出通道数 &gt; 输入通道数）</p>
<p>下采样：s &gt; 1（步长 &gt;1，如s&#x3D;2时分辨率减半）</p>
<p>则任意尺寸的卷积核均可同时实现通道升维和下采样。</p>
<h3 id="2-步长-1是否更快实现下采样？"><a href="#2-步长-1是否更快实现下采样？" class="headerlink" title="2. 步长&gt;1是否更快实现下采样？"></a>2. 步长&gt;1是否更快实现下采样？</h3><p>步长是控制下采样速度的关键参数：</p>
<p>步长&#x3D;1：需结合池化（如MaxPool2d）才能下采样。</p>
<p>步长&gt;1：直接通过卷积一步完成下采样（更高效）。</p>
<p>示例：</p>
<p>输入尺寸 [H, W] &#x3D; [224, 224]</p>
<p>使用3×3卷积，步长&#x3D;2 → 输出尺寸 [112, 112]（下采样速度比步长&#x3D;1快一倍）。</p>
<h3 id="3-不同卷积核的核心用途与选择场景"><a href="#3-不同卷积核的核心用途与选择场景" class="headerlink" title="3.不同卷积核的核心用途与选择场景"></a>3.不同卷积核的核心用途与选择场景</h3><h4 id="1×1卷积核"><a href="#1×1卷积核" class="headerlink" title="1×1卷积核"></a>1×1卷积核</h4><p>核心作用：</p>
<p>通道维度调整：升维（增加特征多样性）或降维（减少计算量）。</p>
<p>跨通道特征融合：通过线性组合不同通道的特征（类似全连接层的局部作用）。</p>
<p>优势：</p>
<p>零空间信息损失：不改变特征图的空间尺寸（步长&#x3D;1时）。</p>
<p>极低的计算量：参数量仅为 C_in × C_out（远小于3×3卷积）。</p>
<p>典型场景：</p>
<p>通道降维：如ResNet的“瓶颈结构”（输入256通道 → 1×1卷积→64通道 → 3×3卷积→64通道 → 1×1卷积→256通道）。</p>
<p>特征融合：如Inception模块中混合不同分支的特征。</p>
<p>轻量化设计：MobileNet用1×1卷积替代全连接层。</p>
<h4 id="3×3卷积核"><a href="#3×3卷积核" class="headerlink" title="3×3卷积核"></a>3×3卷积核</h4><p>核心作用：</p>
<p>空间特征提取：捕捉局部空间模式（如边缘、纹理）。</p>
<p>高效感受野扩展：多个3×3卷积堆叠等效于更大卷积核（如2个3×3 ≈ 5×5感受野）。</p>
<p>优势：</p>
<p>平衡计算量与性能：参数量为 C_in × C_out × 3×3，在多数任务中性价比最高。</p>
<p>典型场景：</p>
<p>基础特征提取：如VGG、ResNet的主干网络。</p>
<p>步长&#x3D;2的下采样：直接通过卷积完成分辨率降低和通道升维。</p>
<h4 id="5×5或更大卷积核"><a href="#5×5或更大卷积核" class="headerlink" title="5×5或更大卷积核"></a>5×5或更大卷积核</h4><p>核心作用：</p>
<p>大感受野特征提取：直接覆盖更大区域，捕捉全局上下文。</p>
<p>劣势：</p>
<p>计算量大：参数量为 C_in × C_out × 5×5，是3×3的约2.8倍。</p>
<p>易过拟合：参数量大时需更多数据支撑。</p>
<p>典型场景：</p>
<p>早期网络设计：如AlexNet的第一层（11×11卷积）。</p>
<p>特定任务需求：如检测任务中需大感受野捕捉远距离关联。</p>
<h3 id="4-不同卷积核的选择策略"><a href="#4-不同卷积核的选择策略" class="headerlink" title="4. 不同卷积核的选择策略"></a>4. 不同卷积核的选择策略</h3><h4 id="优先使用3×3卷积"><a href="#优先使用3×3卷积" class="headerlink" title="优先使用3×3卷积"></a>优先使用3×3卷积</h4><p>原因：</p>
<p>多个3×3卷积堆叠可等效更大卷积核的感受野，但参数量更低（如2层3×3参数量为 2×3²&#x3D;18，1层5×5参数量为 25）。</p>
<p>更深的非线性激活（每层后接ReLU）增强模型表达能力。</p>
<h4 id="需要通道调整时用1×1卷积"><a href="#需要通道调整时用1×1卷积" class="headerlink" title="需要通道调整时用1×1卷积"></a>需要通道调整时用1×1卷积</h4><p>示例：</p>
<p>升维：从64通道→256通道，使用1×1卷积（步长&#x3D;1）。</p>
<p>降维：从256通道→64通道，减少后续3×3卷积的计算量。</p>
<h4 id="大卷积核的替代方案"><a href="#大卷积核的替代方案" class="headerlink" title="大卷积核的替代方案"></a>大卷积核的替代方案</h4><p>空洞卷积（Dilated Convolution）：扩大感受野不增加参数量。</p>
<p>空间注意力机制：动态关注重要区域，替代固定的大卷积核。</p>
<h3 id="5-经典网络中的设计实例"><a href="#5-经典网络中的设计实例" class="headerlink" title="5.经典网络中的设计实例"></a>5.经典网络中的设计实例</h3><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>残差块：使用1×1卷积调整通道数，3×3卷积提取特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ResNet的Bottleneck结构</span></span><br><span class="line">conv1x1(<span class="number">64</span>→<span class="number">256</span>) → conv3x3(<span class="number">256</span>→<span class="number">256</span>) → conv1x1(<span class="number">256</span>→<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception-v4"></a>Inception-v4</h4><p>混合卷积核：并行使用1×1、3×3、5×5卷积，融合多尺度特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inception模块</span></span><br><span class="line">branch1 = conv1x1(<span class="number">64</span>)</span><br><span class="line">branch2 = conv1x1(<span class="number">64</span>) → conv3x3(<span class="number">64</span>)</span><br><span class="line">branch3 = conv1x1(<span class="number">64</span>) → conv5x5(<span class="number">64</span>)</span><br><span class="line">branch4 = MaxPool → conv1x1(<span class="number">64</span>)</span><br><span class="line">concat([branch1, branch2, branch3, branch4])</span><br></pre></td></tr></table></figure>

<h4 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h4><p>深度可分离卷积：先用3×3卷积提取空间特征（通道分离），再用1×1卷积融合通道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 深度可分离卷积</span></span><br><span class="line">depthwise_conv3x3(C_in→C_in, groups=C_in) → conv1x1(C_in→C_out)</span><br></pre></td></tr></table></figure>

<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>所有卷积核均可实现通道升降维和下采样，但不同尺寸的卷积核各有侧重：</p>
<p>1×1卷积：纯通道操作，轻量化调整。</p>
<p>3×3&#x2F;5×5卷积：空间特征提取 + 通道调整。</p>
<p>步长&gt;1的卷积是最高效的下采样方式（一步完成分辨率降低和通道调整）。</p>
<p>设计建议：</p>
<p>优先使用3×3卷积作为主干。</p>
<p>用1×1卷积控制通道数和计算量。</p>
<p>仅在需要大感受野且计算资源充足时使用5×5或更大卷积核。</p>
<h2 id="1-1的卷积的作用核心："><a href="#1-1的卷积的作用核心：" class="headerlink" title="1*1的卷积的作用核心："></a>1*1的卷积的作用核心：</h2><p>表面是是做了通道的切换，实际上是做了特征的组合生成更复杂的特征表示(通道降维)，特征的分离将复杂的特征分离成几个独立的特征(通道升维)</p>
<h3 id="1×1卷积的核心目的"><a href="#1×1卷积的核心目的" class="headerlink" title="1×1卷积的核心目的"></a>1×1卷积的核心目的</h3><h4 id="1-通道维度的灵活调整"><a href="#1-通道维度的灵活调整" class="headerlink" title="1. 通道维度的灵活调整"></a>1. 通道维度的灵活调整</h4><p>降维（减少通道数）：</p>
<p>降低计算复杂度：假设输入为256通道，通过1×1卷积输出64通道，后续的3×3卷积计算量将减少为原来的1&#x2F;4（从256×3×3到64×3×3）。</p>
<p>压缩冗余信息：迫使网络通过少量通道保留关键特征（类似“信息蒸馏”）。</p>
<p>升维（增加通道数）：</p>
<p>扩展特征多样性：为后续层提供更多特征组合的可能性（如不同通道可对应不同抽象层次的特征）。</p>
<p>跨通道交互：通过线性组合不同通道的信息，生成更复杂的特征表示（如颜色+纹理的组合）。</p>
<h4 id="2-跨通道特征融合"><a href="#2-跨通道特征融合" class="headerlink" title="2. 跨通道特征融合"></a>2. 跨通道特征融合</h4><p>线性组合的威力：1×1卷积本质是对所有输入通道的加权求和，输出通道的每个值都是输入通道的全局组合。例如：</p>
<p>输入通道可能分别对应“红色边缘”“蓝色纹理”“绿色斑点”，1×1卷积可以生成“红-蓝组合边缘”“蓝-绿组合纹理”等新特征。</p>
<p>非线性激活前的准备：通过1×1卷积的线性组合，后续的非线性激活（如ReLU）能更高效地捕获复杂模式。</p>
<h4 id="3-网络结构的灵活性"><a href="#3-网络结构的灵活性" class="headerlink" title="3. 网络结构的灵活性"></a>3. 网络结构的灵活性</h4><p>构建“瓶颈”结构（如ResNet）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ResNet Bottleneck</span></span><br><span class="line">输入<span class="number">256</span>通道 → <span class="number">1</span>×<span class="number">1</span>卷积降维至<span class="number">64</span>通道 → <span class="number">3</span>×<span class="number">3</span>卷积处理 → <span class="number">1</span>×<span class="number">1</span>卷积升维回<span class="number">256</span>通道</span><br><span class="line">目的：减少中间<span class="number">3</span>×<span class="number">3</span>卷积的计算量，同时保持输入输出通道一致以支持残差连接。</span><br></pre></td></tr></table></figure>

<h3 id="1×1卷积为何不改变空间尺寸？"><a href="#1×1卷积为何不改变空间尺寸？" class="headerlink" title="1×1卷积为何不改变空间尺寸？"></a>1×1卷积为何不改变空间尺寸？</h3><p>数学操作特性：</p>
<p>卷积核尺寸&#x3D;1×1：每个输出位置仅与输入中同一位置的1×1区域相关，不涉及周围像素。</p>
<p>步长（stride）&#x3D;1：滑动步长为1，确保输出尺寸与输入相同。</p>
<p>填充（padding）&#x3D;0或1：通常通过填充（如padding&#x3D;0）保持尺寸不变。</p>
<p>空间信息的保留：</p>
<p>由于不进行空间聚合（如3×3卷积会混合邻域像素），1×1卷积完全保留原始空间结构，仅操作通道维度。</p>
<h3 id="是否违背信息守恒定律？"><a href="#是否违背信息守恒定律？" class="headerlink" title="是否违背信息守恒定律？"></a>是否违背信息守恒定律？</h3><p>信息守恒的误解：</p>
<p>信息并未丢失，而是重新编码：1×1卷积通过权重矩阵将输入通道的信息投影到新的通道空间中。例如：</p>
<p>输入通道数为3（RGB），通过1×1卷积升维至64通道，每个输出通道是3个原始通道的加权和。</p>
<p>可逆性：若权重矩阵满秩且输出通道数≥输入通道数，理论上可通过另一个1×1卷积还原原始信息（但实际中网络通过训练优化权重，选择性地保留有用信息）。</p>
<p>类似PCA的降维思想：</p>
<p>降维时，1×1卷积类似主成分分析（PCA），通过线性变换保留主要特征，舍弃次要信息（但参数是数据驱动学习而非固定）。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://missonix.github.io">Missonix</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://missonix.github.io/2023/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3/">https://missonix.github.io/2023/03/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%A7%A3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://missonix.github.io" target="_blank">Missonix</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a></div><div class="post-share"><div class="social-share" data-image="/img/Missonix.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/03/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">机器学习基础</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2023/09/29/VPS%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E5%8F%8A%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E4%B8%8E%E6%94%BB%E5%87%BB/" title="VPS网络代理服务器搭建及网络安全与攻击"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">VPS网络代理服务器搭建及网络安全与攻击</div></div><div class="info-2"><div class="info-item-1">基础网络知识参考该网络拓扑图 基础说明：IP地址是电脑主机(服务器)的地址，默认网关是局域网内路由器的地址，如果没有默认网关(路由器)那么 IP地址仅仅只能用于局域网内部的通信；有默认网关(连接路由器)后则主机发送数据(IP地址标识)给网关，网关将数据中转发送至目标主机，网关(路由器)是连接局域网内IP地址与公域网的桥梁。 TCP&#x2F;IP 四层模型四层模型指 应用层、传输层、网络层、数据链路层，除此之外最底部还有一个物理层应用层：HTTP、HTTPS协议及DNS解析在该层完成传输层：端口封装，TCP、UDP握手在该层完成网络层：IP地址封装，虚拟网卡的接入、ICMP在该层数据链路层：MAC地址的封装在该层完成 TCP三次握手(建立连接) 客户端 向 服务器发送消息报文请求连接，关键字段：SYN、ACK、顺序号、确认号、，客户端发送SYN&#x3D;1、ACK&#x3D;0、顺序号&#x3D;x； 服务器收到请求后，回复报文确定可以连接，服务端回复SYN&#x3D;1、ACK&#x3D;1、顺序号&#x3D;y、确认号&#x3D;x+1；...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/09/LLM%E5%BA%94%E7%94%A8%E7%9A%84Bad-Case%E5%9B%9E%E6%B5%81%E7%AD%96%E7%95%A5/" title="LLM应用的Bad Case回流策略"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-09</div><div class="info-item-2">LLM应用的Bad Case回流策略</div></div><div class="info-2"><div class="info-item-1">AI大模型应用产品开发中的Bad Case回流策略设计I. 理解和分类LLM应用中的Bad CaseA. 定义“Bad Case”：超越简单错误在大语言模型（LLM）应用中，“Bad Case”并不仅仅指语法错误或简单的输出不正确；它涵盖了一系列可能削弱用户信任、导致有害结果或指示系统性缺陷的不良行为。这些行为包括输出事实不准确、不相关、带有偏见、有毒有害、不安全，或未能满足用户明确或隐含的意图。此外，它还包括LLM应用未能正确或高效执行其预定任务的功能性失败。 对“Bad Case”的定义必须结合应用的具体目的。例如，一个创意写作助手的模型，其事实性错误可能不如一个医疗诊断辅助工具中的同类错误那样关键 。因此，界定Bad Case需要充分考虑应用场景和潜在风险。 B. LLM失败模式的综合分类法建立一个健全的分类法是系统性处理Bad Case的第一步。此分类法应是多维度的，覆盖LLM性能和行为的各个方面。一个清晰、全面的分类法是任何Bad...</div></div></div></a><a class="pagination-related" href="/2025/01/26/LangChain%E5%9F%BA%E7%A1%80%E4%B8%8ERAG-Agent%E5%BC%80%E5%8F%91/" title="LangChain基础与RAG Agent开发"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-26</div><div class="info-item-2">LangChain基础与RAG Agent开发</div></div><div class="info-2"><div class="info-item-1">LangChain与RAG完整开发指南一、LangChain核心组件1.1 模型实例化123456789# 模型调用示例from langchain_openai import ChatOpenAImodel = ChatOpenAI(    openai_api_key=&quot;sk-xxx&quot;,    openai_api_base=&quot;&quot;,  # 支持国内模型    model_name=&quot;&quot;,    temperature=0.7,  # 控制生成随机性（0-1）    streaming=True    # 启用流式响应) 关键参数说明：  model_name: 支持主流模型如gpt-4、deepseek系列等 temperature: 值越大生成结果越随机 max_tokens: 控制生成内容的最大长度   1.2 提示词模板PromptTemplate 最基础的模板123456from langchain_core.prompts import PromptTemplateprompt_template =...</div></div></div></a><a class="pagination-related" href="/2024/04/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" title="大模型常见问题及解决方案"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-13</div><div class="info-item-2">大模型常见问题及解决方案</div></div><div class="info-2"><div class="info-item-1">大模型常见问题1. 回答不相关 答案相关，但是缺少专业性  答案相关，但是缺少针对性  答案相关，逻辑错误，混乱  抽象概念理解错误 隐含信息识别不足 逻辑一致性差 抗干扰能力差 因果关系判断错误 长文本、多任务    归因 输入大模型的内容质量差 模型的输入不够、不充分，导致它无法解决问题   大模型能力低  解决方案 拆解产品的流程环节 确定问题发生在哪个流程环节  针对输入质量差明确问题： check回答的必需信息，在输入中是否明确 check解答问题，需要哪些步骤 check用户关注哪些生成效果，在输入中是不是有体现 check 大模型的context是否充分  明确问题后解决方案1.缺少知识：RAG2.缺少针对性：增加反问模块3.问题的转化：把输入转化成LLM易于理解的输入  如query to word 针对模型能力差 逻辑推理：增加逻辑推理引擎 逻辑一致性：增加外部记忆组件，对话管理 因果关系：使用专有数据集训练 长文本处理和多任务：层次化处理，类似分批处理 抽象概念：知识图谱嵌入 抗干扰能力差：模型天生的毛病，attention的机制问题  2....</div></div></div></a><a class="pagination-related" href="/2023/03/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="机器学习基础"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-28</div><div class="info-item-2">机器学习基础</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Missonix.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Missonix</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Missonix" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ackerman0919@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%BD%B1%E5%93%8D%E6%84%9F%E5%8F%97%E9%87%8E%E5%A4%A7%E5%B0%8F"><span class="toc-number">1.</span> <span class="toc-text">卷积网络深度影响感受野大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%87%E6%A0%B7"><span class="toc-number">2.</span> <span class="toc-text">采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%B8%8A%E4%B8%8B%E9%87%87%E6%A0%B7%E4%B8%8E%E9%80%9A%E9%81%93%E7%BB%B4%E5%BA%A6"><span class="toc-number">2.1.</span> <span class="toc-text">1. 上下采样与通道维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E4%B8%8E%E4%B8%8B%E9%87%87%E6%A0%B7%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">2.2.</span> <span class="toc-text">2. 卷积与下采样的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%80%9A%E9%81%93%E7%BB%B4%E5%BA%A6%E5%8D%87%E9%99%8D%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="toc-number">2.3.</span> <span class="toc-text">3. 通道维度升降的机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%8B%E9%87%87%E6%A0%B7%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="toc-number">2.4.</span> <span class="toc-text">4. 下采样的目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E9%80%9A%E9%81%93%E5%8D%87%E7%BB%B4%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="toc-number">2.5.</span> <span class="toc-text">5. 通道升维的目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E7%9A%84%E5%85%B8%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.6.</span> <span class="toc-text">6. 网络架构的典型设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">2.7.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">3.</span> <span class="toc-text">卷积核</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%89%80%E6%9C%89%E5%8D%B7%E7%A7%AF%E6%A0%B8%EF%BC%88%E5%A6%823%C3%973%E3%80%815%C3%975%EF%BC%89%E9%83%BD%E8%83%BD%E5%AE%9E%E7%8E%B0%E9%80%9A%E9%81%93%E5%8D%87%E7%BB%B4%E5%92%8C%E4%B8%8B%E9%87%87%E6%A0%B7%E5%90%97%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">1. 所有卷积核（如3×3、5×5）都能实现通道升维和下采样吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%AD%A5%E9%95%BF-1%E6%98%AF%E5%90%A6%E6%9B%B4%E5%BF%AB%E5%AE%9E%E7%8E%B0%E4%B8%8B%E9%87%87%E6%A0%B7%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2. 步长&gt;1是否更快实现下采样？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%8D%E5%90%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E6%A0%B8%E5%BF%83%E7%94%A8%E9%80%94%E4%B8%8E%E9%80%89%E6%8B%A9%E5%9C%BA%E6%99%AF"><span class="toc-number">3.3.</span> <span class="toc-text">3.不同卷积核的核心用途与选择场景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%C3%971%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">3.3.1.</span> <span class="toc-text">1×1卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%C3%973%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">3.3.2.</span> <span class="toc-text">3×3卷积核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%C3%975%E6%88%96%E6%9B%B4%E5%A4%A7%E5%8D%B7%E7%A7%AF%E6%A0%B8"><span class="toc-number">3.3.3.</span> <span class="toc-text">5×5或更大卷积核</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%8D%E5%90%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5"><span class="toc-number">3.4.</span> <span class="toc-text">4. 不同卷积核的选择策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%85%88%E4%BD%BF%E7%94%A83%C3%973%E5%8D%B7%E7%A7%AF"><span class="toc-number">3.4.1.</span> <span class="toc-text">优先使用3×3卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E8%A6%81%E9%80%9A%E9%81%93%E8%B0%83%E6%95%B4%E6%97%B6%E7%94%A81%C3%971%E5%8D%B7%E7%A7%AF"><span class="toc-number">3.4.2.</span> <span class="toc-text">需要通道调整时用1×1卷积</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E6%9B%BF%E4%BB%A3%E6%96%B9%E6%A1%88"><span class="toc-number">3.4.3.</span> <span class="toc-text">大卷积核的替代方案</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.5.</span> <span class="toc-text">5.经典网络中的设计实例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet"><span class="toc-number">3.5.1.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inception-v4"><span class="toc-number">3.5.2.</span> <span class="toc-text">Inception-v4</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MobileNet"><span class="toc-number">3.5.3.</span> <span class="toc-text">MobileNet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">3.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%BD%9C%E7%94%A8%E6%A0%B8%E5%BF%83%EF%BC%9A"><span class="toc-number">4.</span> <span class="toc-text">1*1的卷积的作用核心：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%C3%971%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%A0%B8%E5%BF%83%E7%9B%AE%E7%9A%84"><span class="toc-number">4.1.</span> <span class="toc-text">1×1卷积的核心目的</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%80%9A%E9%81%93%E7%BB%B4%E5%BA%A6%E7%9A%84%E7%81%B5%E6%B4%BB%E8%B0%83%E6%95%B4"><span class="toc-number">4.1.1.</span> <span class="toc-text">1. 通道维度的灵活调整</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%B7%A8%E9%80%9A%E9%81%93%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88"><span class="toc-number">4.1.2.</span> <span class="toc-text">2. 跨通道特征融合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E7%81%B5%E6%B4%BB%E6%80%A7"><span class="toc-number">4.1.3.</span> <span class="toc-text">3. 网络结构的灵活性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1%C3%971%E5%8D%B7%E7%A7%AF%E4%B8%BA%E4%BD%95%E4%B8%8D%E6%94%B9%E5%8F%98%E7%A9%BA%E9%97%B4%E5%B0%BA%E5%AF%B8%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">1×1卷积为何不改变空间尺寸？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%98%AF%E5%90%A6%E8%BF%9D%E8%83%8C%E4%BF%A1%E6%81%AF%E5%AE%88%E6%81%92%E5%AE%9A%E5%BE%8B%EF%BC%9F"><span class="toc-number">4.3.</span> <span class="toc-text">是否违背信息守恒定律？</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/09/%E7%94%B5%E5%95%86%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BC%B0%E7%AD%96%E7%95%A5/" title="电商智能客服系统的评估策略">电商智能客服系统的评估策略</a><time datetime="2025-05-09T13:25:15.000Z" title="发表于 2025-05-09 21:25:15">2025-05-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/09/LLM%E5%BA%94%E7%94%A8%E7%9A%84Bad-Case%E5%9B%9E%E6%B5%81%E7%AD%96%E7%95%A5/" title="LLM应用的Bad Case回流策略">LLM应用的Bad Case回流策略</a><time datetime="2025-05-09T13:23:14.000Z" title="发表于 2025-05-09 21:23:14">2025-05-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/26/LangChain%E5%9F%BA%E7%A1%80%E4%B8%8ERAG-Agent%E5%BC%80%E5%8F%91/" title="LangChain基础与RAG Agent开发">LangChain基础与RAG Agent开发</a><time datetime="2025-01-26T15:01:13.000Z" title="发表于 2025-01-26 23:01:13">2025-01-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/13/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" title="大模型常见问题及解决方案">大模型常见问题及解决方案</a><time datetime="2024-04-13T08:11:38.000Z" title="发表于 2024-04-13 16:11:38">2024-04-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/21/AI%E6%97%B6%E4%BB%A3%E5%AF%B9%E6%96%B0%E4%B8%80%E4%BB%A3%E5%B9%B3%E5%8F%B0%E7%A4%BE%E5%8C%BA%E6%B2%BB%E7%90%86%E8%80%85%E7%9A%84%E8%A6%81%E6%B1%82/" title="AI时代对新一代平台社区治理者的要求">AI时代对新一代平台社区治理者的要求</a><time datetime="2024-03-21T06:39:22.000Z" title="发表于 2024-03-21 14:39:22">2024-03-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Missonix</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div></div></body></html>